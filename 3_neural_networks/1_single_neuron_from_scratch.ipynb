{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Single Neuron From Scratch\n",
        "\n",
        "A neuron is the building block of neural networks!\n",
        "It takes inputs, multiplies by weights, adds bias, and applies an activation function.\n",
        "Let's build one from scratch!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. What is a Neuron?\n",
        "\n",
        "A neuron takes inputs, multiplies by weights, adds a bias, and applies an activation function!\n",
        "Output = activation(weight1 × input1 + weight2 × input2 + ... + bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual calculation: single neuron\n",
        "# Inputs\n",
        "inputs = torch.tensor([2.0, 3.0])\n",
        "\n",
        "# Weights\n",
        "weights = torch.tensor([0.5, 0.3])\n",
        "\n",
        "# Bias\n",
        "bias = 0.1\n",
        "\n",
        "# Compute weighted sum\n",
        "weighted_sum = weights[0] * inputs[0] + weights[1] * inputs[1] + bias\n",
        "\n",
        "print(\"Single Neuron Calculation:\")\n",
        "print(f\"Inputs: {inputs}\")\n",
        "print(f\"Weights: {weights}\")\n",
        "print(f\"Bias: {bias}\")\n",
        "print()\n",
        "print(f\"Weighted sum = {weights[0]} × {inputs[0]} + {weights[1]} × {inputs[1]} + {bias}\")\n",
        "print(f\"Weighted sum = {weighted_sum:.2f}\")\n",
        "print()\n",
        "\n",
        "# Apply activation (sigmoid)\n",
        "activation = 1 / (1 + torch.exp(-weighted_sum))\n",
        "print(f\"After activation (sigmoid): {activation:.4f}\")\n",
        "print()\n",
        "print(\"This is how a single neuron works!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Neuron Class from Scratch\n",
        "\n",
        "Let's create a neuron class that does all this automatically!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "    \"\"\"Single neuron from scratch\"\"\"\n",
        "    \n",
        "    def __init__(self, num_inputs, activation='sigmoid'):\n",
        "        # Initialize weights randomly (small values)\n",
        "        self.weights = torch.randn(num_inputs) * 0.1\n",
        "        # Initialize bias to 0\n",
        "        self.bias = torch.tensor(0.0)\n",
        "        self.activation_name = activation\n",
        "    \n",
        "    def activate(self, x):\n",
        "        \"\"\"Apply activation function\"\"\"\n",
        "        if self.activation_name == 'sigmoid':\n",
        "            return 1 / (1 + torch.exp(-x))\n",
        "        elif self.activation_name == 'relu':\n",
        "            return torch.maximum(torch.tensor(0.0), x)\n",
        "        elif self.activation_name == 'tanh':\n",
        "            return torch.tanh(x)\n",
        "        else:\n",
        "            return x  # No activation (linear)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass: compute output\"\"\"\n",
        "        # Weighted sum: weights · inputs + bias\n",
        "        weighted_sum = torch.dot(self.weights, inputs) + self.bias\n",
        "        # Apply activation\n",
        "        output = self.activate(weighted_sum)\n",
        "        return output\n",
        "\n",
        "# Create a neuron with 2 inputs\n",
        "neuron = Neuron(num_inputs=2, activation='sigmoid')\n",
        "\n",
        "# Test it\n",
        "inputs = torch.tensor([2.0, 3.0])\n",
        "output = neuron.forward(inputs)\n",
        "\n",
        "print(\"Neuron from Scratch:\")\n",
        "print(f\"Inputs: {inputs}\")\n",
        "print(f\"Weights: {neuron.weights}\")\n",
        "print(f\"Bias: {neuron.bias}\")\n",
        "print()\n",
        "print(f\"Output: {output:.4f}\")\n",
        "print()\n",
        "\n",
        "# Show calculation step by step\n",
        "weighted_sum = torch.dot(neuron.weights, inputs) + neuron.bias\n",
        "print(\"Step by step:\")\n",
        "print(f\"1. Weighted sum = weights · inputs + bias\")\n",
        "print(f\"              = {neuron.weights} · {inputs} + {neuron.bias}\")\n",
        "print(f\"              = {weighted_sum:.4f}\")\n",
        "print(f\"2. Activation (sigmoid) = {output:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create neuron with fixed weights for demonstration\n",
        "neuron = Neuron(num_inputs=2, activation='sigmoid')\n",
        "neuron.weights = torch.tensor([0.5, 0.3])\n",
        "neuron.bias = 0.1\n",
        "\n",
        "# Test with different inputs\n",
        "test_inputs = [\n",
        "    torch.tensor([0.0, 0.0]),\n",
        "    torch.tensor([1.0, 0.0]),\n",
        "    torch.tensor([0.0, 1.0]),\n",
        "    torch.tensor([1.0, 1.0]),\n",
        "    torch.tensor([2.0, 3.0]),\n",
        "]\n",
        "\n",
        "print(\"Testing neuron with different inputs:\")\n",
        "print(f\"Weights: {neuron.weights}, Bias: {neuron.bias}\")\n",
        "print()\n",
        "print(\"Input          | Weighted Sum | Output (sigmoid)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for inp in test_inputs:\n",
        "    weighted_sum = torch.dot(neuron.weights, inp) + neuron.bias\n",
        "    output = neuron.forward(inp)\n",
        "    print(f\"{str(inp):14} | {weighted_sum:11.4f} | {output:14.4f}\")\n",
        "\n",
        "# Visualize with 1D input\n",
        "neuron_1d = Neuron(num_inputs=1, activation='sigmoid')\n",
        "neuron_1d.weights = torch.tensor([1.0])\n",
        "neuron_1d.bias = 0.0\n",
        "\n",
        "x = np.linspace(-5, 5, 100)\n",
        "outputs = []\n",
        "\n",
        "for val in x:\n",
        "    inp = torch.tensor([val])\n",
        "    out = neuron_1d.forward(inp)\n",
        "    outputs.append(out.item())\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, outputs, 'b-', linewidth=2, label='Neuron output (sigmoid)')\n",
        "plt.axhline(y=0, color='k', linewidth=0.5, linestyle='--')\n",
        "plt.axvline(x=0, color='k', linewidth=0.5, linestyle='--')\n",
        "plt.xlabel('Input')\n",
        "plt.ylabel('Output')\n",
        "plt.title('Single Neuron Output (Sigmoid Activation)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Notice: Sigmoid activation squashes output to range (0, 1)!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Different Activation Functions\n",
        "\n",
        "Neurons can use different activation functions!\n",
        "Let's see how they affect the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different activation functions\n",
        "activations = ['sigmoid', 'relu', 'tanh', 'linear']\n",
        "neurons = {}\n",
        "\n",
        "for act in activations:\n",
        "    neuron = Neuron(num_inputs=1, activation=act)\n",
        "    neuron.weights = torch.tensor([1.0])\n",
        "    neuron.bias = 0.0\n",
        "    neurons[act] = neuron\n",
        "\n",
        "# Test with same input\n",
        "input_val = torch.tensor([2.0])\n",
        "\n",
        "print(\"Different activation functions with input = 2.0:\")\n",
        "print(f\"Weights: [1.0], Bias: 0.0\")\n",
        "print()\n",
        "\n",
        "for act_name, neuron in neurons.items():\n",
        "    weighted_sum = torch.dot(neuron.weights, input_val) + neuron.bias\n",
        "    output = neuron.forward(input_val)\n",
        "    print(f\"{act_name:8s}: weighted_sum = {weighted_sum:.2f} → output = {output:.4f}\")\n",
        "\n",
        "# Visualize all activations\n",
        "x = np.linspace(-5, 5, 100)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, (act_name, neuron) in enumerate(neurons.items()):\n",
        "    outputs = []\n",
        "    for val in x:\n",
        "        inp = torch.tensor([val])\n",
        "        out = neuron.forward(inp)\n",
        "        outputs.append(out.item())\n",
        "    \n",
        "    axes[idx].plot(x, outputs, linewidth=2)\n",
        "    axes[idx].set_title(f'{act_name.capitalize()} Activation')\n",
        "    axes[idx].set_xlabel('Input')\n",
        "    axes[idx].set_ylabel('Output')\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "    axes[idx].axhline(y=0, color='k', linewidth=0.5, linestyle='--')\n",
        "    axes[idx].axvline(x=0, color='k', linewidth=0.5, linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Key Takeaways\n",
        "\n",
        "**What a neuron does:**\n",
        "- Takes multiple inputs\n",
        "- Multiplies by weights (importance of each input)\n",
        "- Adds bias (shift the output)\n",
        "- Applies activation function (non-linearity)\n",
        "\n",
        "**Key components:**\n",
        "- **Weights**: How important each input is\n",
        "- **Bias**: Shifts the weighted sum\n",
        "- **Activation**: Makes neuron non-linear (sigmoid, ReLU, tanh, etc.)\n",
        "\n",
        "**Forward pass:**\n",
        "1. Weighted sum = weights · inputs + bias\n",
        "2. Output = activation(weighted sum)\n",
        "\n",
        "**Remember:** A single neuron is the building block of neural networks!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
