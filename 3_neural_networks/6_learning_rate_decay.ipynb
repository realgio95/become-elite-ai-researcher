{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Learning Rate and Decay\n",
        "\n",
        "Learning rate controls how big steps we take when updating weights!\n",
        "Too big = overshoot, too small = slow learning.\n",
        "Learning rate decay gradually reduces the learning rate over time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. What is Learning Rate?\n",
        "\n",
        "Learning rate determines step size when updating weights!\n",
        "Weight update: new_weight = old_weight - learning_rate × gradient\n",
        "Bigger learning rate = bigger steps, smaller = smaller steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Gradient descent with different learning rates\n",
        "# Simulate finding minimum of f(x) = (x - 2)²\n",
        "# Gradient: f'(x) = 2(x - 2)\n",
        "# Minimum is at x = 2\n",
        "\n",
        "def gradient(x):\n",
        "    \"\"\"Gradient of f(x) = (x - 2)²\"\"\"\n",
        "    return 2 * (x - 2)\n",
        "\n",
        "# Starting point\n",
        "x_start = 5.0\n",
        "\n",
        "# Different learning rates\n",
        "learning_rates = [0.1, 0.5, 1.0, 2.0]\n",
        "\n",
        "print(\"Gradient Descent with Different Learning Rates:\")\n",
        "print(\"Finding minimum of f(x) = (x - 2)² (minimum at x = 2)\")\n",
        "print(f\"Starting point: x = {x_start}\")\n",
        "print()\n",
        "\n",
        "steps = 10\n",
        "for lr in learning_rates:\n",
        "    x = x_start\n",
        "    path = [x]\n",
        "    \n",
        "    for step in range(steps):\n",
        "        grad = gradient(x)\n",
        "        x = x - lr * grad  # Update: x = x - lr * gradient\n",
        "        path.append(x)\n",
        "    \n",
        "    final_x = path[-1]\n",
        "    print(f\"Learning rate {lr:3.1f}: Final x = {final_x:.4f} (target: 2.0)\")\n",
        "    print(f\"              Steps: {path[:5]} ... → {final_x:.4f}\")\n",
        "\n",
        "# Visualize convergence\n",
        "x_range = np.linspace(-1, 6, 100)\n",
        "f_x = (x_range - 2) ** 2\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(x_range, f_x, 'k-', linewidth=2, label='f(x) = (x-2)²', alpha=0.3)\n",
        "\n",
        "colors = ['b', 'g', 'orange', 'r']\n",
        "for idx, lr in enumerate(learning_rates):\n",
        "    x = x_start\n",
        "    path = [x]\n",
        "    \n",
        "    for step in range(steps):\n",
        "        grad = gradient(x)\n",
        "        x = x - lr * grad\n",
        "        path.append(x)\n",
        "    \n",
        "    plt.plot(path, [(p - 2)**2 for p in path], 'o-', \n",
        "             color=colors[idx], linewidth=2, label=f'LR={lr}', alpha=0.7)\n",
        "\n",
        "plt.axvline(x=2, color='k', linewidth=1, linestyle='--', label='Minimum (x=2)')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.title('Gradient Descent with Different Learning Rates')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.xlim(-1, 6)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nObservations:\")\n",
        "print(\"- LR too small (0.1): Converges slowly but safely\")\n",
        "print(\"- LR moderate (0.5): Good balance\")\n",
        "print(\"- LR too large (2.0): Overshoots, oscillates, may diverge!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Simple Gradient Descent\n",
        "\n",
        "Basic gradient descent updates weights using learning rate!\n",
        "Let's implement it!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple gradient descent example\n",
        "# Goal: Find weights that minimize loss = (prediction - target)²\n",
        "# Simple model: prediction = weight × input\n",
        "\n",
        "# Data\n",
        "inputs = torch.tensor([1.0, 2.0, 3.0])\n",
        "targets = torch.tensor([2.0, 4.0, 6.0])  # Target: weight = 2.0\n",
        "\n",
        "# Initialize weight\n",
        "weight = torch.tensor(1.0, requires_grad=False)  # Start at 1.0\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "print(\"Simple Gradient Descent:\")\n",
        "print(\"Model: prediction = weight × input\")\n",
        "print(\"Target weight: 2.0\")\n",
        "print(f\"Initial weight: {weight.item():.2f}\")\n",
        "print(f\"Learning rate: {learning_rate}\")\n",
        "print()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    predictions = weight * inputs\n",
        "    \n",
        "    # Loss (mean squared error)\n",
        "    loss = ((predictions - targets) ** 2).mean()\n",
        "    \n",
        "    # Gradient (manual calculation)\n",
        "    # d(loss)/d(weight) = 2 * mean((weight*input - target) * input)\n",
        "    gradient = 2 * ((weight * inputs - targets) * inputs).mean()\n",
        "    \n",
        "    # Update weight\n",
        "    weight = weight - learning_rate * gradient\n",
        "    \n",
        "    if epoch < 5 or epoch % 2 == 0:\n",
        "        print(f\"Epoch {epoch:2d}: weight = {weight.item():.4f}, loss = {loss.item():.4f}, gradient = {gradient.item():.4f}\")\n",
        "\n",
        "print(f\"\\nFinal weight: {weight.item():.4f} (target: 2.0)\")\n",
        "print(f\"Converged: {abs(weight.item() - 2.0) < 0.01}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
