{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Building a Layer from Scratch\n",
        "\n",
        "A layer contains multiple neurons!\n",
        "Each neuron in a layer processes the same inputs but with different weights.\n",
        "Let's build a layer from scratch!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. What is a Layer?\n",
        "\n",
        "A layer is a collection of neurons!\n",
        "All neurons receive the same inputs, but each has its own weights and bias.\n",
        "Output of a layer = vector (one output per neuron).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual calculation: layer with 2 neurons, 2 inputs each\n",
        "# Inputs\n",
        "inputs = torch.tensor([2.0, 3.0])\n",
        "\n",
        "# Layer weights: 2 neurons × 2 inputs = 2×2 matrix\n",
        "# Row i = weights for neuron i\n",
        "weights = torch.tensor([[0.5, 0.3],   # Neuron 1 weights\n",
        "                       [0.2, 0.4]])    # Neuron 2 weights\n",
        "\n",
        "# Biases: one per neuron\n",
        "biases = torch.tensor([0.1, 0.2])\n",
        "\n",
        "print(\"Layer with 2 neurons, 2 inputs:\")\n",
        "print(f\"Inputs: {inputs}\")\n",
        "print(f\"Weights shape: {weights.shape} (neurons × inputs)\")\n",
        "print(f\"Weights:\\n{weights}\")\n",
        "print(f\"Biases: {biases}\")\n",
        "print()\n",
        "\n",
        "# Forward pass: each neuron computes its output\n",
        "# This is just matrix multiplication!\n",
        "outputs = inputs @ weights.T + biases  # weights.T because we need (inputs × weights)\n",
        "\n",
        "print(\"Forward pass:\")\n",
        "print(f\"Outputs = inputs @ weights.T + biases\")\n",
        "print(f\"        = {inputs} @ {weights.T} + {biases}\")\n",
        "print(f\"        = {outputs}\")\n",
        "print()\n",
        "\n",
        "# Apply activation (sigmoid) to each output\n",
        "activation = torch.sigmoid(outputs)\n",
        "print(f\"After activation (sigmoid): {activation}\")\n",
        "\n",
        "# Show step by step for each neuron\n",
        "print(\"\\nStep by step for each neuron:\")\n",
        "for i in range(2):\n",
        "    weighted_sum = torch.dot(weights[i], inputs) + biases[i]\n",
        "    activated = torch.sigmoid(weighted_sum)\n",
        "    print(f\"Neuron {i+1}: {weights[i]} · {inputs} + {biases[i]} = {weighted_sum:.4f} → {activated:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Layer Class from Scratch\n",
        "\n",
        "Let's create a layer class that contains multiple neurons!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    \"\"\"Layer with multiple neurons\"\"\"\n",
        "    \n",
        "    def __init__(self, num_inputs, num_neurons, activation='sigmoid'):\n",
        "        # Weight matrix: (num_neurons, num_inputs)\n",
        "        # Each row is weights for one neuron\n",
        "        self.weights = torch.randn(num_neurons, num_inputs) * 0.1\n",
        "        # Bias vector: one per neuron\n",
        "        self.biases = torch.randn(num_neurons) * 0.1\n",
        "        self.activation_name = activation\n",
        "    \n",
        "    def activate(self, x):\n",
        "        \"\"\"Apply activation function\"\"\"\n",
        "        if self.activation_name == 'sigmoid':\n",
        "            return torch.sigmoid(x)\n",
        "        elif self.activation_name == 'relu':\n",
        "            return torch.relu(x)\n",
        "        elif self.activation_name == 'tanh':\n",
        "            return torch.tanh(x)\n",
        "        else:\n",
        "            return x  # No activation (linear)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass: compute layer output\"\"\"\n",
        "        # Matrix multiplication: inputs @ weights.T + biases\n",
        "        # inputs: (batch_size, num_inputs) or (num_inputs,)\n",
        "        # weights: (num_neurons, num_inputs)\n",
        "        # output: (batch_size, num_neurons) or (num_neurons,)\n",
        "        \n",
        "        if inputs.dim() == 1:\n",
        "            # Single input\n",
        "            weighted_sum = inputs @ self.weights.T + self.biases\n",
        "        else:\n",
        "            # Batch of inputs\n",
        "            weighted_sum = inputs @ self.weights.T + self.biases\n",
        "        \n",
        "        # Apply activation\n",
        "        output = self.activate(weighted_sum)\n",
        "        return output\n",
        "\n",
        "# Create a layer: 3 neurons, 2 inputs each\n",
        "layer = Layer(num_inputs=2, num_neurons=3, activation='sigmoid')\n",
        "\n",
        "# Test it\n",
        "inputs = torch.tensor([2.0, 3.0])\n",
        "outputs = layer.forward(inputs)\n",
        "\n",
        "print(\"Layer from Scratch:\")\n",
        "print(f\"Number of inputs: 2\")\n",
        "print(f\"Number of neurons: 3\")\n",
        "print(f\"Inputs: {inputs}\")\n",
        "print()\n",
        "print(f\"Weights shape: {layer.weights.shape} (neurons × inputs)\")\n",
        "print(f\"Weights:\\n{layer.weights}\")\n",
        "print()\n",
        "print(f\"Biases: {layer.biases}\")\n",
        "print()\n",
        "print(f\"Outputs: {outputs}\")\n",
        "print(f\"Output shape: {outputs.shape} (one output per neuron)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a layer\n",
        "layer = Layer(num_inputs=2, num_neurons=3, activation='sigmoid')\n",
        "\n",
        "# Single input\n",
        "single_input = torch.tensor([1.0, 2.0])\n",
        "single_output = layer.forward(single_input)\n",
        "\n",
        "print(\"Single input:\")\n",
        "print(f\"Input shape: {single_input.shape}\")\n",
        "print(f\"Output shape: {single_output.shape}\")\n",
        "print(f\"Output: {single_output}\")\n",
        "print()\n",
        "\n",
        "# Batch of inputs (3 samples)\n",
        "batch_inputs = torch.tensor([[1.0, 2.0],\n",
        "                             [3.0, 4.0],\n",
        "                             [5.0, 6.0]])\n",
        "\n",
        "batch_outputs = layer.forward(batch_inputs)\n",
        "\n",
        "print(\"Batch of inputs:\")\n",
        "print(f\"Input shape: {batch_inputs.shape} (batch_size=3, num_inputs=2)\")\n",
        "print(f\"Output shape: {batch_outputs.shape} (batch_size=3, num_neurons=3)\")\n",
        "print(f\"Outputs:\\n{batch_outputs}\")\n",
        "print()\n",
        "\n",
        "print(\"Notice: Layer processes all inputs in batch simultaneously!\")\n",
        "print(\"Each row of output corresponds to one input in batch.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualizing Layer Output\n",
        "\n",
        "Let's see how a layer transforms inputs!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple layer for visualization\n",
        "layer = Layer(num_inputs=1, num_neurons=3, activation='sigmoid')\n",
        "# Set weights manually for demonstration\n",
        "layer.weights = torch.tensor([[1.0],   # Neuron 1: steep\n",
        "                             [0.5],    # Neuron 2: moderate\n",
        "                             [-1.0]])  # Neuron 3: negative slope\n",
        "layer.biases = torch.tensor([0.0, 0.0, 0.0])\n",
        "\n",
        "# Test with different inputs\n",
        "x = np.linspace(-5, 5, 100)\n",
        "outputs_per_neuron = [[], [], []]\n",
        "\n",
        "for val in x:\n",
        "    inp = torch.tensor([val])\n",
        "    out = layer.forward(inp)\n",
        "    for i in range(3):\n",
        "        outputs_per_neuron[i].append(out[i].item())\n",
        "\n",
        "# Visualize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "for i in range(3):\n",
        "    plt.plot(x, outputs_per_neuron[i], linewidth=2, label=f'Neuron {i+1}')\n",
        "\n",
        "plt.xlabel('Input')\n",
        "plt.ylabel('Output')\n",
        "plt.title('Layer with 3 Neurons (1 input each)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.axhline(y=0, color='k', linewidth=0.5, linestyle='--')\n",
        "plt.axvline(x=0, color='k', linewidth=0.5, linestyle='--')\n",
        "plt.show()\n",
        "\n",
        "print(\"Notice: Each neuron in the layer produces a different output!\")\n",
        "print(\"Together they create a richer representation of the input.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Key Takeaways\n",
        "\n",
        "**What a layer is:**\n",
        "- Collection of neurons that process the same inputs\n",
        "- Each neuron has its own weights and bias\n",
        "- Output is a vector (one value per neuron)\n",
        "\n",
        "**Key components:**\n",
        "- **Weight matrix**: (num_neurons, num_inputs) - each row = one neuron's weights\n",
        "- **Bias vector**: (num_neurons,) - one bias per neuron\n",
        "- **Activation function**: Applied to each neuron's output\n",
        "\n",
        "**Forward pass:**\n",
        "1. Weighted sum = inputs @ weights.T + biases (matrix multiplication!)\n",
        "2. Output = activation(weighted sum)\n",
        "\n",
        "**Batch processing:**\n",
        "- Can process multiple inputs at once\n",
        "- Input shape: (batch_size, num_inputs)\n",
        "- Output shape: (batch_size, num_neurons)\n",
        "\n",
        "**Remember:** Layers transform inputs from one dimension to another!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
