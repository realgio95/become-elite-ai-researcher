{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Adam Optimizer\n",
        "\n",
        "Adam (Adaptive Moment Estimation) is a popular optimizer!\n",
        "It adapts learning rate for each parameter based on past gradients.\n",
        "Combines ideas from momentum and RMSprop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. What is Adam?\n",
        "\n",
        "Adam adapts learning rate per parameter!\n",
        "It keeps track of:\n",
        "- **First moment (m)**: Exponential moving average of gradients (momentum)\n",
        "- **Second moment (v)**: Exponential moving average of squared gradients\n",
        "\n",
        "Then updates: weight = weight - lr × m_hat / (√v_hat + eps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdamOptimizer:\n",
        "    \"\"\"Adam optimizer from scratch\"\"\"\n",
        "    \n",
        "    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "        \"\"\"\n",
        "        params: dictionary of parameters {name: tensor}\n",
        "        lr: learning rate\n",
        "        beta1: decay rate for first moment (momentum)\n",
        "        beta2: decay rate for second moment\n",
        "        eps: small constant for numerical stability\n",
        "        \"\"\"\n",
        "        self.params = params\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        \n",
        "        # Initialize moments (all zeros)\n",
        "        self.m = {name: torch.zeros_like(param) for name, param in params.items()}\n",
        "        self.v = {name: torch.zeros_like(param) for name, param in params.items()}\n",
        "        \n",
        "        # Time step\n",
        "        self.t = 0\n",
        "    \n",
        "    def step(self, gradients):\n",
        "        \"\"\"\n",
        "        gradients: dictionary of gradients {name: gradient}\n",
        "        Update parameters using Adam\n",
        "        \"\"\"\n",
        "        self.t += 1\n",
        "        \n",
        "        for name in self.params.keys():\n",
        "            grad = gradients[name]\n",
        "            \n",
        "            # Update first moment (momentum)\n",
        "            self.m[name] = self.beta1 * self.m[name] + (1 - self.beta1) * grad\n",
        "            \n",
        "            # Update second moment (squared gradients)\n",
        "            self.v[name] = self.beta2 * self.v[name] + (1 - self.beta2) * (grad ** 2)\n",
        "            \n",
        "            # Bias correction\n",
        "            m_hat = self.m[name] / (1 - self.beta1 ** self.t)\n",
        "            v_hat = self.v[name] / (1 - self.beta2 ** self.t)\n",
        "            \n",
        "            # Update parameter\n",
        "            self.params[name] = self.params[name] - self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)\n",
        "\n",
        "# Example: Optimize simple function f(x) = (x - 2)²\n",
        "# Gradient: f'(x) = 2(x - 2)\n",
        "\n",
        "# Initialize\n",
        "x = torch.tensor(5.0)\n",
        "params = {'x': x}\n",
        "optimizer = AdamOptimizer(params, lr=0.1)\n",
        "\n",
        "print(\"Adam Optimizer:\")\n",
        "print(f\"Initial x: {x.item():.2f}\")\n",
        "print(f\"Target: x = 2.0 (minimum of f(x) = (x-2)²)\")\n",
        "print()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(20):\n",
        "    # Compute gradient\n",
        "    grad = 2 * (params['x'] - 2)\n",
        "    gradients = {'x': grad}\n",
        "    \n",
        "    # Update with Adam\n",
        "    optimizer.step(gradients)\n",
        "    \n",
        "    # Compute loss\n",
        "    loss = (params['x'] - 2) ** 2\n",
        "    \n",
        "    if epoch < 5 or epoch % 5 == 0:\n",
        "        print(f\"Epoch {epoch:2d}: x = {params['x'].item():.4f}, \"\n",
        "              f\"grad = {grad.item():.4f}, loss = {loss.item():.6f}, \"\n",
        "              f\"m = {optimizer.m['x'].item():.4f}, v = {optimizer.v['x'].item():.4f}\")\n",
        "\n",
        "print(f\"\\nFinal x: {params['x'].item():.4f} (target: 2.0)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
